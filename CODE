CNN model code:
import torch
import torch.nn as nn
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
# Load the CSV file
data = pd.read_csv('fiepath')
# Split the data into training and testing sets
train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)
# Prepare the data
X_train = torch.from_numpy(train_data.drop(['outcome'], axis=1).values).float()
y_train = torch.from_numpy(train_data['outcome'].values).float()
X_test = torch.from_numpy(test_data.drop(['outcome'], axis=1).values).float()
y_test = torch.from_numpy(test_data['outcome'].values).float()
# Define the CNN architecture
class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding=1)
        self.pool = nn.MaxPool1d(kernel_size=2)
        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=2)
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(in_features=32* 9, out_features=64)
        self.fc2 = nn.Linear(in_features=64, out_features=1)
    def forward(self, x):
        x = self.conv1(x)
        x = nn.functional.relu(x)
        x = self.pool(x)
        x = self.conv2(x)
        x = nn.functional.relu(x)
        x = self.pool(x)
        x = x.view(-1, 32*9)
        x = self.fc1(x)
        x = nn.functional.relu(x)
        x = self.fc2(x)
        return x
# Initialize the model
model = CNN()
# Move your model and data to the GPU
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model.to(device)
X_train = X_train.to(device)
y_train = y_train.to(device)
X_test = X_test.to(device)
y_test = y_test.to(device)
print(X_train.size(),y_train.size())
# Define the loss function and optimizer
criterion = nn.BCEWithLogitsLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  
# Train the model
num_epochs = ###
for epoch in range(num_epochs):
    running_loss = 0.0
    for i in range(len(X_train)):
        inputs = X_train[i].unsqueeze(0).unsqueeze(0)
        labels = y_train[i].unsqueeze(0).unsqueeze(0)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch + 1, num_epochs, running_loss / len(X_train)))
    # Evaluate the model
    with torch.no_grad():
        correct = 0
        total = 0
        for i in range(len(X_test)):
            inputs = X_test[i].unsqueeze(0).unsqueeze(0)
            labels = y_test[i].unsqueeze(0).unsqueeze(0)
            outputs = model(inputs)
            predicted = (outputs > 0.5).float()
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
        print('Accuracy of the network on the test data: {} %'.format(100 * correct / total))


Neural network model code:

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from sklearn.model_selection import train_test_split
data = pd.read_csv('filepath')
X = data.iloc[:, :-1].values
y = data.iloc[:, -1].values
y = torch.tensor(y)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train, X_test, y_train, y_test = map(torch.tensor, (X_train, X_test, y_train, y_test))
class Net(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(X_train.shape[1], 16)
        self.fc2 = nn.Linear(16, 8)
        self.fc3 = nn.Linear(8, 2)
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = torch.softmax(self.fc3(x), dim=-1)
        return x
net = Net()
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(net.parameters(), lr=0.001)
for epoch in range(2000):
    optimizer.zero_grad()  
    outputs = net(X_train.float())
    loss = criterion(outputs, y_train.long())
    loss.backward()
    optimizer.step()
    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, ###, loss.item()))
with torch.no_grad():
    outputs = net(X_test.float())
    _, predicted = torch.max(outputs.data, 1)
accuracy = (predicted == y_test).sum().item() / len(y_test)
print('Accuracy:', accuracy)



RandomForestClassfier model code:

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
data = pd.read_csv("filepath")
X = data.iloc[:, :-1].values
y = data.iloc[:, -1].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
rf = RandomForestClassifier()
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy: {}%".format(accuracy*100))



SVMCLassfier model code:

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import pandas as pd
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset, DataLoader
class SVM(nn.Module):
    def __init__(self, input_dim):
        super(SVM, self).__init__()
        self.linear = nn.Linear(input_dim, 1)
        
    def forward(self, x):
        out = self.linear(x)
        return out
def hinge_loss(outputs, targets):
    loss = torch.mean(torch.clamp(1 - targets * outputs, min=0))
    return loss
data = pd.read_csv("filepath")
X = data.drop('outcome', axis=1).values
y = data['outcome'].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
class IVFDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)
        
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, index):
        return self.X[index], self.y[index]
train_data = IVFDataset(X_train, y_train)
test_data = IVFDataset(X_test, y_test)
train_loader = DataLoader(train_data, batch_size=64, shuffle=True)
test_loader = DataLoader(test_data, batch_size=64, shuffle=False)
input_dim = X_train.shape[1]
svm_model = SVM(input_dim)
optimizer = optim.SGD(svm_model.parameters(), lr=0.01)
num_epochs = 100
for epoch in range(num_epochs):
    for i, (inputs, targets) in enumerate(train_loader):
        optimizer.zero_grad()
        outputs = svm_model(inputs)
        loss = hinge_loss(outputs, targets)
        loss.backward()
        optimizer.step()
svm_model.eval()
correct = 0
total = 0
with torch.no_grad():
    for inputs, targets in test_loader:
        outputs = svm_model(inputs)
        predicted = torch.sign(outputs)
        total += targets.size(0)
        correct += (predicted == targets).sum().item()
accuracy = 100 * correct / total
print("Accuracy: {}%".format(accuracy))



DecisionTree Classifer model code:

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
data = pd.read_csv("filepath")
X = data.iloc[:, :-1].values
y = data.iloc[:, -1].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
clf = DecisionTreeClassifier()
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy: {}%".format(accuracy*100))



Naive Bayes classifier model code:

import pandas as pd
import torch
from torch.utils.data import TensorDataset, DataLoader
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt
data = pd.read_csv("filepath")
X = data.iloc[:, :-1].values
y = data.iloc[:, -1].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train, X_test, y_train, y_test = map(torch.tensor, (X_train, X_test, y_train, y_test))
class NaiveBayes(nn.Module):
    def __init__(self, n_features, n_classes):
        super(NaiveBayes, self).__init__()
        self.prior = nn.Parameter(torch.zeros(n_classes))
        self.likelihood = nn.Parameter(torch.zeros(n_features, n_classes))
        
    def forward(self, x):
        log_prior = self.prior.unsqueeze(0)
        log_likelihood = torch.matmul(x, self.likelihood)
        log_prob = log_prior + log_likelihood
        prob = torch.exp(log_prob)
        prob_norm = prob / prob.sum(dim=1, keepdim=True)
        return prob_norm

n_features = X_train.shape[1]
n_classes = len(torch.unique(y_train))
nb = NaiveBayes(n_features, n_classes)

criterion = nn.NLLLoss()
optimizer = optim.SGD(nb.parameters(), lr=0.001)

train_ds = TensorDataset(X_train.float(), y_train.long())
train_dl = DataLoader(train_ds, batch_size=10, shuffle=True)

loss_list = []
for epoch in range(200):
    running_loss = 0.0
    for xb, yb in train_dl:
        optimizer.zero_grad()
        prob = nb(xb)
        loss = criterion(torch.log(prob), yb)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
    epoch_loss = running_loss / len(train_dl)
    loss_list.append(epoch_loss)
    print("Epoch {}: Loss = {:.4f}".format(epoch+1, epoch_loss))

with torch.no_grad():
    nb.eval()
    prob = nb(X_test.float())
    pred = torch.argmax(prob, dim=1)
    accuracy = (pred == y_test).float().mean()
    print("Accuracy: {}%".format(accuracy*100))
    
    fpr, tpr, _ = roc_curve(y_test, prob[:, 1])
    roc_auc = auc(fpr, tpr)

plt.figure()
lw = 2
plt.plot(fpr, tpr, color='darkorange',
         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()
losses = []
for epoch in range(200):
    running_loss = 0.0
    for xb, yb in train_dl:
        optimizer.zero_grad()
        prob = nb(xb)
        loss = criterion(torch.log(prob), yb)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
    losses.append(running_loss)
    print("Epoch {}: Loss = {:.4f}".format(epoch+1, running_loss))

plt.plot(losses)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss')
plt.show()


